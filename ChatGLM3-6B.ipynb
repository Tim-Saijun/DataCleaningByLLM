{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected operating system as Ubuntu/focal.\n",
      "Checking for curl...\n",
      "Detected curl...\n",
      "Checking for gpg...\n",
      "Detected gpg...\n",
      "Detected apt version as 2.0.6\n",
      "Running apt-get update... done.\n",
      "Installing apt-transport-https... ^C\n"
     ]
    }
   ],
   "source": [
    "# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "# !sudo apt-get install git-lfs\n",
    "# # åˆ‡æ¢åˆ°ç›®æ ‡æ–‡ä»¶å¤¹\n",
    "# !git lfs install\n",
    "# !git lfs pull\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®˜æ–¹Demo\n",
    "[DemoğŸ”—](https://github.com/THUDM/ChatGLM2-6B)\n",
    "å·ç§°æ‹¥æœ‰10Bä»¥ä¸‹æœ€å¼ºçš„åŸºç¡€æ¨¡å‹ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆFunction Callï¼‰ã€ä»£ç æ‰§è¡Œï¼ˆCode Interpreterï¼‰ã€Agent ä»»åŠ¡ç­‰åŠŸèƒ½ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "Fetching 1 files:   0%|                                   | 0/1 [00:00<?, ?it/s]downloading https://hf-mirror.com/THUDM/chatglm3-6b/resolve/e46a14881eae613281abbd266ee918e93a56018f/pytorch_model-00001-of-00007.bin to /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/blobs/4d5567466e89625dbd10e51c69a02982f233a10108cf232a379defdbb065ae0b.incomplete\n",
      "\n",
      "pytorch_model-00001-of-00007.bin:   0%|             | 0.00/1.83G [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   1%|    | 10.5M/1.83G [00:00<01:57, 15.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   1%|    | 21.0M/1.83G [00:00<01:15, 23.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   2%|    | 31.5M/1.83G [00:01<01:05, 27.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   2%|    | 41.9M/1.83G [00:01<01:03, 27.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   3%|    | 52.4M/1.83G [00:01<00:55, 31.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   3%|â–   | 62.9M/1.83G [00:02<00:54, 32.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   4%|â–   | 73.4M/1.83G [00:02<00:49, 35.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   5%|â–   | 83.9M/1.83G [00:02<00:47, 36.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   5%|â–   | 94.4M/1.83G [00:02<00:46, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   6%|â–    | 105M/1.83G [00:03<00:52, 32.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   6%|â–    | 115M/1.83G [00:03<00:51, 33.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   7%|â–    | 126M/1.83G [00:04<00:56, 30.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   7%|â–    | 136M/1.83G [00:04<00:54, 31.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   8%|â–    | 147M/1.83G [00:04<00:49, 33.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   9%|â–    | 157M/1.83G [00:04<00:47, 35.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   9%|â–    | 168M/1.83G [00:05<00:45, 36.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  10%|â–    | 178M/1.83G [00:05<00:43, 37.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  10%|â–Œ    | 189M/1.83G [00:05<00:43, 38.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  11%|â–Œ    | 199M/1.83G [00:05<00:42, 38.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  11%|â–Œ    | 210M/1.83G [00:06<00:42, 38.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  12%|â–Œ    | 220M/1.83G [00:06<00:48, 33.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  13%|â–‹    | 231M/1.83G [00:06<00:47, 33.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  13%|â–‹    | 241M/1.83G [00:07<00:51, 30.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  14%|â–‹    | 252M/1.83G [00:07<00:51, 30.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  14%|â–‹    | 262M/1.83G [00:08<00:47, 32.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  15%|â–‹    | 273M/1.83G [00:08<00:45, 34.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  15%|â–Š    | 283M/1.83G [00:08<00:44, 34.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  16%|â–Š    | 294M/1.83G [00:08<00:42, 36.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  17%|â–Š    | 304M/1.83G [00:09<00:43, 35.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  17%|â–Š    | 315M/1.83G [00:09<00:43, 34.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  18%|â–‰    | 325M/1.83G [00:09<00:42, 35.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  18%|â–‰    | 336M/1.83G [00:10<00:42, 35.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  19%|â–‰    | 346M/1.83G [00:10<00:41, 36.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  20%|â–‰    | 357M/1.83G [00:10<00:43, 33.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  20%|â–ˆ    | 367M/1.83G [00:11<00:49, 29.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  21%|â–ˆ    | 377M/1.83G [00:11<00:49, 29.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  21%|â–ˆ    | 388M/1.83G [00:11<00:45, 31.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  22%|â–ˆ    | 398M/1.83G [00:12<00:42, 33.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  22%|â–ˆ    | 409M/1.83G [00:12<00:39, 35.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  23%|â–ˆâ–   | 419M/1.83G [00:12<00:39, 35.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  24%|â–ˆâ–   | 430M/1.83G [00:12<00:39, 35.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  24%|â–ˆâ–   | 440M/1.83G [00:13<00:36, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  25%|â–ˆâ–   | 451M/1.83G [00:13<00:36, 38.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  25%|â–ˆâ–   | 461M/1.83G [00:13<00:36, 37.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  26%|â–ˆâ–   | 472M/1.83G [00:13<00:36, 37.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  26%|â–ˆâ–   | 482M/1.83G [00:14<00:37, 36.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  27%|â–ˆâ–   | 493M/1.83G [00:14<00:38, 34.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  28%|â–ˆâ–   | 503M/1.83G [00:14<00:38, 34.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  28%|â–ˆâ–   | 514M/1.83G [00:15<00:37, 34.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  29%|â–ˆâ–   | 524M/1.83G [00:15<00:38, 33.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  29%|â–ˆâ–   | 535M/1.83G [00:15<00:38, 33.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  30%|â–ˆâ–   | 545M/1.83G [00:16<00:36, 35.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  30%|â–ˆâ–Œ   | 556M/1.83G [00:16<00:35, 36.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  31%|â–ˆâ–Œ   | 566M/1.83G [00:16<00:35, 36.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  32%|â–ˆâ–Œ   | 577M/1.83G [00:16<00:33, 37.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  32%|â–ˆâ–Œ   | 587M/1.83G [00:17<00:33, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  33%|â–ˆâ–‹   | 598M/1.83G [00:17<00:32, 37.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  33%|â–ˆâ–‹   | 608M/1.83G [00:17<00:31, 38.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  34%|â–ˆâ–‹   | 619M/1.83G [00:18<00:33, 36.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  34%|â–ˆâ–‹   | 629M/1.83G [00:18<00:32, 36.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  35%|â–ˆâ–‹   | 640M/1.83G [00:18<00:41, 28.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  36%|â–ˆâ–Š   | 650M/1.83G [00:19<00:37, 31.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  36%|â–ˆâ–Š   | 661M/1.83G [00:19<00:35, 32.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  37%|â–ˆâ–Š   | 671M/1.83G [00:19<00:35, 32.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  37%|â–ˆâ–Š   | 682M/1.83G [00:20<00:33, 34.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  38%|â–ˆâ–‰   | 692M/1.83G [00:20<00:33, 34.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  38%|â–ˆâ–‰   | 703M/1.83G [00:20<00:30, 36.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  39%|â–ˆâ–‰   | 713M/1.83G [00:20<00:32, 34.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  40%|â–ˆâ–‰   | 724M/1.83G [00:21<00:31, 35.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  40%|â–ˆâ–ˆ   | 734M/1.83G [00:21<00:30, 35.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  41%|â–ˆâ–ˆ   | 744M/1.83G [00:21<00:30, 35.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  41%|â–ˆâ–ˆ   | 755M/1.83G [00:22<00:28, 37.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  42%|â–ˆâ–ˆ   | 765M/1.83G [00:22<00:27, 38.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  42%|â–ˆâ–ˆ   | 776M/1.83G [00:22<00:26, 39.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  43%|â–ˆâ–ˆâ–  | 786M/1.83G [00:22<00:27, 37.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  44%|â–ˆâ–ˆâ–  | 797M/1.83G [00:23<00:26, 38.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  44%|â–ˆâ–ˆâ–  | 807M/1.83G [00:23<00:26, 38.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  45%|â–ˆâ–ˆâ–  | 818M/1.83G [00:23<00:26, 37.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  45%|â–ˆâ–ˆâ–  | 828M/1.83G [00:23<00:26, 38.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  46%|â–ˆâ–ˆâ–  | 839M/1.83G [00:24<00:25, 38.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  46%|â–ˆâ–ˆâ–  | 849M/1.83G [00:24<00:24, 39.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  47%|â–ˆâ–ˆâ–  | 860M/1.83G [00:24<00:24, 39.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  48%|â–ˆâ–ˆâ–  | 870M/1.83G [00:25<00:23, 40.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  48%|â–ˆâ–ˆâ–  | 881M/1.83G [00:25<00:23, 39.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  49%|â–ˆâ–ˆâ–  | 891M/1.83G [00:25<00:23, 40.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  49%|â–ˆâ–ˆâ–  | 902M/1.83G [00:25<00:22, 40.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  50%|â–ˆâ–ˆâ–  | 912M/1.83G [00:26<00:23, 38.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  50%|â–ˆâ–ˆâ–Œ  | 923M/1.83G [00:26<00:22, 39.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  51%|â–ˆâ–ˆâ–Œ  | 933M/1.83G [00:26<00:25, 34.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  52%|â–ˆâ–ˆâ–Œ  | 944M/1.83G [00:27<00:25, 34.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  52%|â–ˆâ–ˆâ–Œ  | 954M/1.83G [00:27<00:24, 35.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  53%|â–ˆâ–ˆâ–‹  | 965M/1.83G [00:27<00:23, 37.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  53%|â–ˆâ–ˆâ–‹  | 975M/1.83G [00:27<00:22, 38.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  54%|â–ˆâ–ˆâ–‹  | 986M/1.83G [00:28<00:22, 37.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  55%|â–ˆâ–ˆâ–‹  | 996M/1.83G [00:28<00:21, 38.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  55%|â–ˆâ–ˆâ– | 1.01G/1.83G [00:28<00:21, 37.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  56%|â–ˆâ–ˆâ– | 1.02G/1.83G [00:28<00:22, 36.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  56%|â–ˆâ–ˆâ– | 1.03G/1.83G [00:29<00:21, 37.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  57%|â–ˆâ–ˆâ– | 1.04G/1.83G [00:29<00:20, 38.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  57%|â–ˆâ–ˆâ– | 1.05G/1.83G [00:29<00:20, 38.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  58%|â–ˆâ–ˆâ– | 1.06G/1.83G [00:30<00:19, 38.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  59%|â–ˆâ–ˆâ– | 1.07G/1.83G [00:30<00:19, 39.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  59%|â–ˆâ–ˆâ– | 1.08G/1.83G [00:30<00:20, 36.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  60%|â–ˆâ–ˆâ– | 1.09G/1.83G [00:31<00:28, 26.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  60%|â–ˆâ–ˆâ– | 1.10G/1.83G [00:31<00:27, 26.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  61%|â–ˆâ–ˆâ– | 1.11G/1.83G [00:32<00:30, 23.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  61%|â–ˆâ–ˆâ– | 1.12G/1.83G [00:32<00:27, 26.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  62%|â–ˆâ–ˆâ– | 1.13G/1.83G [00:32<00:24, 28.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  63%|â–ˆâ–ˆâ–Œ | 1.14G/1.83G [00:33<00:23, 28.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  63%|â–ˆâ–ˆâ–Œ | 1.15G/1.83G [00:33<00:21, 31.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  64%|â–ˆâ–ˆâ–Œ | 1.16G/1.83G [00:33<00:19, 34.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  64%|â–ˆâ–ˆâ–Œ | 1.17G/1.83G [00:33<00:19, 34.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  65%|â–ˆâ–ˆâ–Œ | 1.18G/1.83G [00:34<00:17, 35.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  65%|â–ˆâ–ˆâ–Œ | 1.20G/1.83G [00:34<00:17, 37.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  66%|â–ˆâ–ˆâ–‹ | 1.21G/1.83G [00:34<00:16, 36.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  67%|â–ˆâ–ˆâ–‹ | 1.22G/1.83G [00:35<00:16, 38.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  67%|â–ˆâ–ˆâ–‹ | 1.23G/1.83G [00:35<00:15, 38.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  68%|â–ˆâ–ˆâ–‹ | 1.24G/1.83G [00:35<00:15, 39.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  68%|â–ˆâ–ˆâ–‹ | 1.25G/1.83G [00:35<00:15, 37.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  69%|â–ˆâ–ˆâ–Š | 1.26G/1.83G [00:36<00:15, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  69%|â–ˆâ–ˆâ–Š | 1.27G/1.83G [00:36<00:15, 37.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  70%|â–ˆâ–ˆâ–Š | 1.28G/1.83G [00:36<00:16, 32.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  71%|â–ˆâ–ˆâ–Š | 1.29G/1.83G [00:37<00:15, 33.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  71%|â–ˆâ–ˆâ–Š | 1.30G/1.83G [00:37<00:15, 35.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  72%|â–ˆâ–ˆâ–Š | 1.31G/1.83G [00:37<00:17, 30.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  72%|â–ˆâ–ˆâ–‰ | 1.32G/1.83G [00:38<00:15, 33.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  73%|â–ˆâ–ˆâ–‰ | 1.33G/1.83G [00:38<00:14, 33.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  73%|â–ˆâ–ˆâ–‰ | 1.34G/1.83G [00:38<00:14, 33.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  74%|â–ˆâ–ˆâ–‰ | 1.35G/1.83G [00:39<00:13, 34.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  75%|â–ˆâ–ˆâ–‰ | 1.36G/1.83G [00:39<00:13, 34.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  75%|â–ˆâ–ˆâ–ˆ | 1.37G/1.83G [00:39<00:12, 36.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  76%|â–ˆâ–ˆâ–ˆ | 1.38G/1.83G [00:40<00:15, 28.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  76%|â–ˆâ–ˆâ–ˆ | 1.39G/1.83G [00:40<00:14, 29.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  77%|â–ˆâ–ˆâ–ˆ | 1.41G/1.83G [00:40<00:13, 32.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  77%|â–ˆâ–ˆâ–ˆ | 1.42G/1.83G [00:40<00:12, 33.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  78%|â–ˆâ–ˆâ–ˆ | 1.43G/1.83G [00:41<00:11, 35.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  79%|â–ˆâ–ˆâ–ˆâ–| 1.44G/1.83G [00:41<00:11, 35.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  79%|â–ˆâ–ˆâ–ˆâ–| 1.45G/1.83G [00:41<00:10, 36.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  80%|â–ˆâ–ˆâ–ˆâ–| 1.46G/1.83G [00:42<00:10, 37.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  80%|â–ˆâ–ˆâ–ˆâ–| 1.47G/1.83G [00:42<00:09, 38.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  81%|â–ˆâ–ˆâ–ˆâ–| 1.48G/1.83G [00:42<00:08, 38.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  81%|â–ˆâ–ˆâ–ˆâ–| 1.49G/1.83G [00:42<00:08, 38.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  82%|â–ˆâ–ˆâ–ˆâ–| 1.50G/1.83G [00:43<00:08, 39.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  83%|â–ˆâ–ˆâ–ˆâ–| 1.51G/1.83G [00:43<00:07, 39.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  83%|â–ˆâ–ˆâ–ˆâ–| 1.52G/1.83G [00:43<00:07, 40.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  84%|â–ˆâ–ˆâ–ˆâ–| 1.53G/1.83G [00:43<00:07, 39.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  84%|â–ˆâ–ˆâ–ˆâ–| 1.54G/1.83G [00:44<00:07, 39.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  85%|â–ˆâ–ˆâ–ˆâ–| 1.55G/1.83G [00:44<00:07, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  85%|â–ˆâ–ˆâ–ˆâ–| 1.56G/1.83G [00:44<00:07, 37.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  86%|â–ˆâ–ˆâ–ˆâ–| 1.57G/1.83G [00:45<00:06, 37.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  87%|â–ˆâ–ˆâ–ˆâ–| 1.58G/1.83G [00:45<00:07, 33.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  87%|â–ˆâ–ˆâ–ˆâ–| 1.59G/1.83G [00:45<00:06, 33.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  88%|â–ˆâ–ˆâ–ˆâ–Œ| 1.60G/1.83G [00:46<00:07, 31.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  88%|â–ˆâ–ˆâ–ˆâ–Œ| 1.61G/1.83G [00:46<00:07, 30.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  89%|â–ˆâ–ˆâ–ˆâ–Œ| 1.63G/1.83G [00:46<00:06, 31.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  89%|â–ˆâ–ˆâ–ˆâ–Œ| 1.64G/1.83G [00:47<00:06, 30.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  90%|â–ˆâ–ˆâ–ˆâ–Œ| 1.65G/1.83G [00:47<00:07, 24.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  91%|â–ˆâ–ˆâ–ˆâ–‹| 1.66G/1.83G [00:48<00:06, 27.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  91%|â–ˆâ–ˆâ–ˆâ–‹| 1.67G/1.83G [00:48<00:05, 28.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 1.68G/1.83G [00:48<00:05, 29.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 1.69G/1.83G [00:49<00:04, 31.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  93%|â–ˆâ–ˆâ–ˆâ–‹| 1.70G/1.83G [00:49<00:03, 33.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  94%|â–ˆâ–ˆâ–ˆâ–‹| 1.71G/1.83G [00:49<00:03, 35.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  94%|â–ˆâ–ˆâ–ˆâ–Š| 1.72G/1.83G [00:49<00:02, 36.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  95%|â–ˆâ–ˆâ–ˆâ–Š| 1.73G/1.83G [00:50<00:02, 37.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  95%|â–ˆâ–ˆâ–ˆâ–Š| 1.74G/1.83G [00:50<00:02, 37.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  96%|â–ˆâ–ˆâ–ˆâ–Š| 1.75G/1.83G [00:50<00:02, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  96%|â–ˆâ–ˆâ–ˆâ–Š| 1.76G/1.83G [00:50<00:01, 36.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  97%|â–ˆâ–ˆâ–ˆâ–‰| 1.77G/1.83G [00:51<00:01, 36.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  98%|â–ˆâ–ˆâ–ˆâ–‰| 1.78G/1.83G [00:51<00:01, 34.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  98%|â–ˆâ–ˆâ–ˆâ–‰| 1.79G/1.83G [00:51<00:00, 35.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 1.80G/1.83G [00:52<00:00, 35.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 1.81G/1.83G [00:52<00:00, 36.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 1.83G/1.83G [00:52<00:00, 34.7MB/s]\u001b[A\n",
      "Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:54<00:00, 54.12s/it]\n",
      "/root/THUDM/chatglm3-6b\n"
     ]
    }
   ],
   "source": [
    "# å›½å†…ä½¿ç”¨æ­¤å‘½ä»¤é•œåƒä¸‹è½½æ¨¡å‹æ–‡ä»¶\n",
    "!export HF_ENDPOINT=https://hf-mirror.com && huggingface-cli download --resume-download --local-dir-use-symlinks False THUDM/chatglm3-6b --local-dir /root/THUDM/chatglm3-6b --include pytorch_model-00001-of-00007.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/lm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:05<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n",
      "æ™šä¸Šç¡ä¸ç€è§‰å¯èƒ½ä¼šè®©äººæ„Ÿåˆ°ç„¦è™‘å’Œç–²åŠ³ï¼Œè¿™é‡Œæœ‰ä¸€äº›å»ºè®®å¯èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼š\n",
      "\n",
      "1. å°è¯•æ”¾æ¾ï¼šæ·±å‘¼å¸ã€å†¥æƒ³ã€æ¸è¿›æ€§è‚Œè‚‰æ¾å¼›ç­‰æ”¾æ¾æŠ€å·§å¯ä»¥å¸®åŠ©ä½ å‡è½»ç„¦è™‘å’Œå‹åŠ›ï¼Œä¿ƒè¿›ç¡çœ ã€‚\n",
      "2. è§„å¾‹ä½œæ¯ï¼šä¿æŒè§„å¾‹çš„ä½œæ¯æ—¶é—´ï¼Œæ¯å¤©å°½é‡åœ¨ç›¸åŒçš„æ—¶é—´ä¸ŠåºŠç¡è§‰å’Œèµ·åºŠï¼Œæœ‰åŠ©äºè°ƒæ•´èº«ä½“çš„ç”Ÿç‰©é’Ÿã€‚\n",
      "3. å‡å°‘ä½¿ç”¨ç”µå­è®¾å¤‡ï¼šç”µå­å±å¹•äº§ç”Ÿçš„è“å…‰å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ï¼Œå»ºè®®åœ¨ç¡å‰ä¸€å°æ—¶åœæ­¢ä½¿ç”¨ç”µå­è®¾å¤‡ã€‚\n",
      "4. åˆ›é€ ä¸€ä¸ªè‰¯å¥½çš„ç¡çœ ç¯å¢ƒï¼šä¿æŒå®‰é™ã€èˆ’é€‚ã€é»‘æš—çš„ç¡çœ ç¯å¢ƒï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨çœ¼ç½©æˆ–è€³å¡ç­‰è¾…åŠ©å·¥å…·ã€‚\n",
      "5. é¿å…é¥®ç”¨å’–å•¡å› å’Œé…’ç²¾ï¼šå’–å•¡å› å’Œé…’ç²¾éƒ½å¯èƒ½å¹²æ‰°ç¡çœ ï¼Œå»ºè®®é¿å…åœ¨æ™šä¸Šé¥®ç”¨ã€‚\n",
      "6. è¿›è¡Œé€‚å½“çš„è¿åŠ¨ï¼šé€‚å½“çš„è¿åŠ¨å¯ä»¥å¸®åŠ©ä½ æ”¾æ¾èº«å¿ƒï¼Œä½†é¿å…åœ¨ç¡å‰è¿›è¡Œå‰§çƒˆè¿åŠ¨ã€‚\n",
      "7. é¥®é£Ÿè°ƒæ•´ï¼šé¿å…åœ¨ç¡å‰ä¸¤ä¸‰å°æ—¶å†…æ‘„å…¥è¿‡å¤šçš„é£Ÿç‰©å’Œé¥®æ–™ï¼Œå°¤å…¶æ˜¯å«å’–å•¡å› å’Œç³–åˆ†çš„é£Ÿç‰©ã€‚\n",
      "\n",
      "å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½ä¸èƒ½æ”¹å–„ä½ çš„å¤±çœ é—®é¢˜ï¼Œå¯èƒ½éœ€è¦è€ƒè™‘å’¨è¯¢ä¸“ä¸šçš„åŒ»ç”Ÿæˆ–å¿ƒç†å­¦å®¶ï¼Œè·å–æ›´å…·ä½“çš„å»ºè®®å’Œæ²»ç–—æ–¹æ¡ˆã€‚\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "model = AutoModel.from_pretrained(\"/root/THUDM/chatglm3-6b\", trust_remote_code=True, device='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/THUDM/chatglm3-6b\", trust_remote_code=True)\n",
    "\n",
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¸…æ´—\n",
    "### è‹±æ–‡Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given data has an error in the third column. The country \"China\" is associated with the city \"Tokyo\", which is incorrect. The correct country for Tokyo is \"Japan\". Therefore, the error in the data is 1 for this line. The updated data is:\n",
      "\n",
      "1,New York,United States\n",
      "2,London,United Kingdom\n",
      "3,Paris,France\n",
      "4,Tokyo,Japan\n",
      "5,Sydney,Australia\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Example output: [0,0,0,0,1]\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data provided is correct and does not contain any errors. Therefore, the desired output would be [0, 0, 1, 0, 1].\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Example output: [0,0,1,0,1]\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Example output: [0,0,1,0,1]\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given data has errors in the following line:\n",
      "4,Tokyo,China\n",
      "\n",
      "The country name \"China\" is incorrect, as Tokyo is actually located in Japan.\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Example output: [0,0,1,0,1]\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Example output: [0,0,1,0,1]\\n\\\n",
    "Notice: return the number list only, do not return any string or explaination.\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Notice: return the number list only, do not return any string or explaination.\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸­æ–‡prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def check_data(data):\n",
      "    result = []\n",
      "    for row in data:\n",
      "        if len(row) != 3:\n",
      "            result.append(1)\n",
      "        elif not (isinstance(row[0], int) and isinstance(row[1], str) and isinstance(row[2], str)):\n",
      "            result.append(1)\n",
      "        else:\n",
      "            result.append(0)\n",
      "    return result\n",
      "\n",
      "data = [\n",
      "    (1, \"New York\", \"United States\"),\n",
      "    (2, \"London\", \"United Kingdom\"),\n",
      "    (3, \"Paris\", \"France\"),\n",
      "    (4, \"Tokyo\", \"China\"),\n",
      "    (5, \"Sydney\", \"Australia\")\n",
      "]\n",
      "\n",
      "print(check_data(data))\n"
     ]
    }
   ],
   "source": [
    "dete = \"å¯¹ç»™å®šçš„æ•°æ®è¿›è¡Œé”™è¯¯æ£€æŸ¥ï¼Œæ•°æ®çš„ç¬¬ä¸€åˆ—æ˜¯æ•°å­—ï¼Œç¬¬äºŒåˆ—æ˜¯åŸå¸‚ï¼Œç¬¬ä¸‰åˆ—æ˜¯ç¬¬äºŒåˆ—æ‰€å±çš„å›½å®¶ã€‚\\n\\\n",
    "å½“æŸä¸€è¡Œæ•°æ®æ²¡æœ‰é”™è¯¯æ—¶ï¼Œè¿”å›ç»“æœæ˜¯0ï¼Œå½“æŸä¸€è¡Œæ•°æ®æœ‰é”™è¯¯æ—¶ï¼Œè¿”å›ç»“æœæ˜¯1ã€‚\\n\\\n",
    "ä¸€ä¸ªè¿”å›çš„ä¾‹å­: [0,0,1,0,1]\\n\\\n",
    "æ³¨æ„ï¼šæœ€ç»ˆç»“æœåªè¦è¿”å›ä¸€ä¸ªæ•°å­—ç»„æˆçš„åˆ—è¡¨ï¼Œä¸éœ€è¦è¿”å›ä»»ä½•è§£é‡Šæˆ–è€…ä»»ä½•å­—ç¬¦å†…å®¹.\\n\\\n",
    "æ•°æ®:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¥½çš„ï¼Œæˆ‘ä¼šå¯¹ç»™å®šçš„æ•°æ®è¿›è¡Œé”™è¯¯æ£€æŸ¥ã€‚è¯·æä¾›æ‚¨çš„æ•°æ®ã€‚\n"
     ]
    }
   ],
   "source": [
    "dete = \"You are a helpful assisant. ä½ çš„ä»»åŠ¡æ˜¯å¯¹ç»™å®šçš„æ•°æ®è¿›è¡Œé”™è¯¯æ£€æŸ¥ï¼Œæ•°æ®çš„ç¬¬ä¸€åˆ—æ˜¯æ•°å­—ï¼Œç¬¬äºŒåˆ—æ˜¯åŸå¸‚ï¼Œç¬¬ä¸‰åˆ—æ˜¯ç¬¬äºŒåˆ—æ‰€å±çš„å›½å®¶ã€‚\\n\\\n",
    "å½“æŸä¸€è¡Œæ•°æ®æ²¡æœ‰é”™è¯¯æ—¶ï¼Œè¿”å›ç»“æœæ˜¯0ï¼Œå½“æŸä¸€è¡Œæ•°æ®æœ‰é”™è¯¯æ—¶ï¼Œè¿”å›ç»“æœæ˜¯1ã€‚\\n\\\n",
    "ä¸€ä¸ªè¿”å›çš„ä¾‹å­: [0,0,1,0,1]\\n\\\n",
    "æ³¨æ„ï¼šæœ€ç»ˆç»“æœåªè¦è¿”å›ä¸€ä¸ªæ•°å­—ç»„æˆçš„åˆ—è¡¨ï¼Œä¸éœ€è¦è¿”å›ä»»ä½•è§£é‡Šæˆ–è€…ä»»ä½•å­—ç¬¦å†…å®¹.\\n\\\n",
    "æ•°æ®:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def check_data(data):\n",
      "    result = []\n",
      "    for row in data:\n",
      "        if len(row) != 3:\n",
      "            result.append(1)\n",
      "        elif not (isinstance(row[0], int) and isinstance(row[1], str) and isinstance(row[2], str)):\n",
      "            result.append(1)\n",
      "        else:\n",
      "            result.append(0)\n",
      "    return result\n"
     ]
    }
   ],
   "source": [
    "dete = \"You are a helpful assisant. ä½ çš„ä»»åŠ¡æ˜¯å¯¹ç»™å®šçš„æ•°æ®è¿›è¡Œé”™è¯¯æ£€æŸ¥ï¼Œæ•°æ®çš„ç¬¬ä¸€åˆ—æ˜¯æ•°å­—ï¼Œç¬¬äºŒåˆ—æ˜¯åŸå¸‚ï¼Œç¬¬ä¸‰åˆ—æ˜¯ç¬¬äºŒåˆ—æ‰€å±çš„å›½å®¶ã€‚\\n\\\n",
    "å½“æŸä¸€è¡Œæ•°æ®æ²¡æœ‰é”™è¯¯æ—¶ï¼Œè¿”å›ç»“æœæ˜¯0ï¼Œå½“æŸä¸€è¡Œæ•°æ®æœ‰é”™è¯¯æ—¶ï¼Œè¿”å›ç»“æœæ˜¯1ã€‚\\n\\\n",
    "ä¸€ä¸ªè¿”å›çš„ä¾‹å­: [0,0,1,0,1]\\n\\\n",
    "æ³¨æ„ï¼šæœ€ç»ˆç»“æœåªè¦è¿”å›ä¸€ä¸ªæ•°å­—ç»„æˆçš„åˆ—è¡¨ï¼Œä¸éœ€è¦è¿”å›ä»»ä½•è§£é‡Šæˆ–è€…ä»»ä½•å­—ç¬¦å†…å®¹.\\n\\\n",
    "æ•°æ®:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def check_data(data):\n",
      "    result = []\n",
      "    for row in data:\n",
      "        if row[1] == 'United Kingdom':\n",
      "            result.append(0)\n",
      "        elif row[1] == 'France':\n",
      "            result.append(1)\n",
      "        else:\n",
      "            result.append(0)\n",
      "    return result\n",
      "\n",
      "data = [\n",
      "    [1, 'New York', 'United States'],\n",
      "    [2, 'London', 'United Kingdom'],\n",
      "    [3, 'Paris', 'France'],\n",
      "    [4, 'Tokyo', 'China'],\n",
      "    [5, 'Sydney', 'Australia']\n",
      "]\n",
      "\n",
      "print(check_data(data))\n"
     ]
    }
   ],
   "source": [
    "dete = \"You are a helpful assisant. ä½ çš„ä»»åŠ¡æ˜¯å¯¹ç»™å®šçš„æ•°æ®è¿›è¡Œé”™è¯¯æ£€æŸ¥ï¼Œæ•°æ®çš„ç¬¬ä¸€åˆ—æ˜¯æ•°å­—ï¼Œç¬¬äºŒåˆ—æ˜¯åŸå¸‚ï¼Œç¬¬ä¸‰åˆ—æ˜¯ç¬¬äºŒåˆ—æ‰€å±çš„å›½å®¶ã€‚\\n\\\n",
    "æ•°æ®å¦‚ä¸‹:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\\n\\\n",
    "å½“æŸä¸€è¡Œæ•°æ®æ²¡æœ‰é”™è¯¯æ—¶ï¼Œè¿”å›ç»“æœæ˜¯0ï¼Œå½“æŸä¸€è¡Œæ•°æ®æœ‰é”™è¯¯æ—¶ï¼Œè¿”å›ç»“æœæ˜¯1ã€‚\\n\\\n",
    "ä¸€ä¸ªè¿”å›çš„ä¾‹å­: [0,0,1,0,1]\\n\\\n",
    "æ³¨æ„ï¼šæœ€ç»ˆç»“æœåªè¦è¿”å›ä¸€ä¸ªæ•°å­—ç»„æˆçš„åˆ—è¡¨ï¼Œä¸éœ€è¦è¿”å›ä»»ä½•è§£é‡Šæˆ–è€…ä»»ä½•å­—ç¬¦å†…å®¹.\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "å¯ä»¥å®Œæˆç®€å•çš„æ¸…æ´—ä»»åŠ¡ï¼Œèƒ½æ­£ç¡®è¿”å›detectionçš„ç»“æœä¸è§£é‡Šã€‚\n",
    "ä½†ä¸èƒ½å®Œå…¨ç†è§£ä»»åŠ¡è¦æ±‚ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯promptçš„åŸå› ï¼Œè¿”å›ç»“æœä¸åˆè¦æ±‚ã€‚\n",
    "ä¸­æ–‡promptç”šè‡³ç›´æ¥è¿”å›ä»£ç ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
