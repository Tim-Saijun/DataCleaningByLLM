{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected operating system as Ubuntu/focal.\n",
      "Checking for curl...\n",
      "Detected curl...\n",
      "Checking for gpg...\n",
      "Detected gpg...\n",
      "Detected apt version as 2.0.6\n",
      "Running apt-get update... done.\n",
      "Installing apt-transport-https... ^C\n"
     ]
    }
   ],
   "source": [
    "# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "# !sudo apt-get install git-lfs\n",
    "# # 切换到目标文件夹\n",
    "# !git lfs install\n",
    "# !git lfs pull\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 官方Demo\n",
    "[Demo🔗](https://github.com/THUDM/ChatGLM2-6B)\n",
    "号称拥有10B以下最强的基础模型，支持工具调用（Function Call）、代码执行（Code Interpreter）、Agent 任务等功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "Fetching 1 files:   0%|                                   | 0/1 [00:00<?, ?it/s]downloading https://hf-mirror.com/THUDM/chatglm3-6b/resolve/e46a14881eae613281abbd266ee918e93a56018f/pytorch_model-00001-of-00007.bin to /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/blobs/4d5567466e89625dbd10e51c69a02982f233a10108cf232a379defdbb065ae0b.incomplete\n",
      "\n",
      "pytorch_model-00001-of-00007.bin:   0%|             | 0.00/1.83G [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   1%|    | 10.5M/1.83G [00:00<01:57, 15.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   1%|    | 21.0M/1.83G [00:00<01:15, 23.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   2%|    | 31.5M/1.83G [00:01<01:05, 27.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   2%|    | 41.9M/1.83G [00:01<01:03, 27.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   3%|    | 52.4M/1.83G [00:01<00:55, 31.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   3%|▏   | 62.9M/1.83G [00:02<00:54, 32.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   4%|▏   | 73.4M/1.83G [00:02<00:49, 35.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   5%|▏   | 83.9M/1.83G [00:02<00:47, 36.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   5%|▏   | 94.4M/1.83G [00:02<00:46, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   6%|▎    | 105M/1.83G [00:03<00:52, 32.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   6%|▎    | 115M/1.83G [00:03<00:51, 33.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   7%|▎    | 126M/1.83G [00:04<00:56, 30.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   7%|▎    | 136M/1.83G [00:04<00:54, 31.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   8%|▍    | 147M/1.83G [00:04<00:49, 33.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   9%|▍    | 157M/1.83G [00:04<00:47, 35.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:   9%|▍    | 168M/1.83G [00:05<00:45, 36.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  10%|▍    | 178M/1.83G [00:05<00:43, 37.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  10%|▌    | 189M/1.83G [00:05<00:43, 38.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  11%|▌    | 199M/1.83G [00:05<00:42, 38.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  11%|▌    | 210M/1.83G [00:06<00:42, 38.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  12%|▌    | 220M/1.83G [00:06<00:48, 33.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  13%|▋    | 231M/1.83G [00:06<00:47, 33.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  13%|▋    | 241M/1.83G [00:07<00:51, 30.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  14%|▋    | 252M/1.83G [00:07<00:51, 30.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  14%|▋    | 262M/1.83G [00:08<00:47, 32.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  15%|▋    | 273M/1.83G [00:08<00:45, 34.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  15%|▊    | 283M/1.83G [00:08<00:44, 34.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  16%|▊    | 294M/1.83G [00:08<00:42, 36.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  17%|▊    | 304M/1.83G [00:09<00:43, 35.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  17%|▊    | 315M/1.83G [00:09<00:43, 34.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  18%|▉    | 325M/1.83G [00:09<00:42, 35.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  18%|▉    | 336M/1.83G [00:10<00:42, 35.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  19%|▉    | 346M/1.83G [00:10<00:41, 36.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  20%|▉    | 357M/1.83G [00:10<00:43, 33.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  20%|█    | 367M/1.83G [00:11<00:49, 29.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  21%|█    | 377M/1.83G [00:11<00:49, 29.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  21%|█    | 388M/1.83G [00:11<00:45, 31.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  22%|█    | 398M/1.83G [00:12<00:42, 33.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  22%|█    | 409M/1.83G [00:12<00:39, 35.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  23%|█▏   | 419M/1.83G [00:12<00:39, 35.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  24%|█▏   | 430M/1.83G [00:12<00:39, 35.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  24%|█▏   | 440M/1.83G [00:13<00:36, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  25%|█▏   | 451M/1.83G [00:13<00:36, 38.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  25%|█▎   | 461M/1.83G [00:13<00:36, 37.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  26%|█▎   | 472M/1.83G [00:13<00:36, 37.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  26%|█▎   | 482M/1.83G [00:14<00:37, 36.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  27%|█▎   | 493M/1.83G [00:14<00:38, 34.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  28%|█▍   | 503M/1.83G [00:14<00:38, 34.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  28%|█▍   | 514M/1.83G [00:15<00:37, 34.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  29%|█▍   | 524M/1.83G [00:15<00:38, 33.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  29%|█▍   | 535M/1.83G [00:15<00:38, 33.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  30%|█▍   | 545M/1.83G [00:16<00:36, 35.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  30%|█▌   | 556M/1.83G [00:16<00:35, 36.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  31%|█▌   | 566M/1.83G [00:16<00:35, 36.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  32%|█▌   | 577M/1.83G [00:16<00:33, 37.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  32%|█▌   | 587M/1.83G [00:17<00:33, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  33%|█▋   | 598M/1.83G [00:17<00:32, 37.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  33%|█▋   | 608M/1.83G [00:17<00:31, 38.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  34%|█▋   | 619M/1.83G [00:18<00:33, 36.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  34%|█▋   | 629M/1.83G [00:18<00:32, 36.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  35%|█▋   | 640M/1.83G [00:18<00:41, 28.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  36%|█▊   | 650M/1.83G [00:19<00:37, 31.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  36%|█▊   | 661M/1.83G [00:19<00:35, 32.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  37%|█▊   | 671M/1.83G [00:19<00:35, 32.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  37%|█▊   | 682M/1.83G [00:20<00:33, 34.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  38%|█▉   | 692M/1.83G [00:20<00:33, 34.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  38%|█▉   | 703M/1.83G [00:20<00:30, 36.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  39%|█▉   | 713M/1.83G [00:20<00:32, 34.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  40%|█▉   | 724M/1.83G [00:21<00:31, 35.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  40%|██   | 734M/1.83G [00:21<00:30, 35.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  41%|██   | 744M/1.83G [00:21<00:30, 35.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  41%|██   | 755M/1.83G [00:22<00:28, 37.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  42%|██   | 765M/1.83G [00:22<00:27, 38.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  42%|██   | 776M/1.83G [00:22<00:26, 39.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  43%|██▏  | 786M/1.83G [00:22<00:27, 37.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  44%|██▏  | 797M/1.83G [00:23<00:26, 38.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  44%|██▏  | 807M/1.83G [00:23<00:26, 38.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  45%|██▏  | 818M/1.83G [00:23<00:26, 37.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  45%|██▎  | 828M/1.83G [00:23<00:26, 38.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  46%|██▎  | 839M/1.83G [00:24<00:25, 38.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  46%|██▎  | 849M/1.83G [00:24<00:24, 39.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  47%|██▎  | 860M/1.83G [00:24<00:24, 39.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  48%|██▍  | 870M/1.83G [00:25<00:23, 40.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  48%|██▍  | 881M/1.83G [00:25<00:23, 39.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  49%|██▍  | 891M/1.83G [00:25<00:23, 40.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  49%|██▍  | 902M/1.83G [00:25<00:22, 40.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  50%|██▍  | 912M/1.83G [00:26<00:23, 38.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  50%|██▌  | 923M/1.83G [00:26<00:22, 39.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  51%|██▌  | 933M/1.83G [00:26<00:25, 34.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  52%|██▌  | 944M/1.83G [00:27<00:25, 34.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  52%|██▌  | 954M/1.83G [00:27<00:24, 35.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  53%|██▋  | 965M/1.83G [00:27<00:23, 37.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  53%|██▋  | 975M/1.83G [00:27<00:22, 38.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  54%|██▋  | 986M/1.83G [00:28<00:22, 37.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  55%|██▋  | 996M/1.83G [00:28<00:21, 38.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  55%|██▏ | 1.01G/1.83G [00:28<00:21, 37.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  56%|██▏ | 1.02G/1.83G [00:28<00:22, 36.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  56%|██▏ | 1.03G/1.83G [00:29<00:21, 37.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  57%|██▎ | 1.04G/1.83G [00:29<00:20, 38.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  57%|██▎ | 1.05G/1.83G [00:29<00:20, 38.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  58%|██▎ | 1.06G/1.83G [00:30<00:19, 38.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  59%|██▎ | 1.07G/1.83G [00:30<00:19, 39.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  59%|██▎ | 1.08G/1.83G [00:30<00:20, 36.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  60%|██▍ | 1.09G/1.83G [00:31<00:28, 26.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  60%|██▍ | 1.10G/1.83G [00:31<00:27, 26.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  61%|██▍ | 1.11G/1.83G [00:32<00:30, 23.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  61%|██▍ | 1.12G/1.83G [00:32<00:27, 26.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  62%|██▍ | 1.13G/1.83G [00:32<00:24, 28.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  63%|██▌ | 1.14G/1.83G [00:33<00:23, 28.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  63%|██▌ | 1.15G/1.83G [00:33<00:21, 31.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  64%|██▌ | 1.16G/1.83G [00:33<00:19, 34.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  64%|██▌ | 1.17G/1.83G [00:33<00:19, 34.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  65%|██▌ | 1.18G/1.83G [00:34<00:17, 35.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  65%|██▌ | 1.20G/1.83G [00:34<00:17, 37.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  66%|██▋ | 1.21G/1.83G [00:34<00:16, 36.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  67%|██▋ | 1.22G/1.83G [00:35<00:16, 38.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  67%|██▋ | 1.23G/1.83G [00:35<00:15, 38.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  68%|██▋ | 1.24G/1.83G [00:35<00:15, 39.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  68%|██▋ | 1.25G/1.83G [00:35<00:15, 37.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  69%|██▊ | 1.26G/1.83G [00:36<00:15, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  69%|██▊ | 1.27G/1.83G [00:36<00:15, 37.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  70%|██▊ | 1.28G/1.83G [00:36<00:16, 32.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  71%|██▊ | 1.29G/1.83G [00:37<00:15, 33.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  71%|██▊ | 1.30G/1.83G [00:37<00:15, 35.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  72%|██▊ | 1.31G/1.83G [00:37<00:17, 30.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  72%|██▉ | 1.32G/1.83G [00:38<00:15, 33.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  73%|██▉ | 1.33G/1.83G [00:38<00:14, 33.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  73%|██▉ | 1.34G/1.83G [00:38<00:14, 33.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  74%|██▉ | 1.35G/1.83G [00:39<00:13, 34.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  75%|██▉ | 1.36G/1.83G [00:39<00:13, 34.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  75%|███ | 1.37G/1.83G [00:39<00:12, 36.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  76%|███ | 1.38G/1.83G [00:40<00:15, 28.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  76%|███ | 1.39G/1.83G [00:40<00:14, 29.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  77%|███ | 1.41G/1.83G [00:40<00:13, 32.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  77%|███ | 1.42G/1.83G [00:40<00:12, 33.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  78%|███ | 1.43G/1.83G [00:41<00:11, 35.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  79%|███▏| 1.44G/1.83G [00:41<00:11, 35.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  79%|███▏| 1.45G/1.83G [00:41<00:10, 36.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  80%|███▏| 1.46G/1.83G [00:42<00:10, 37.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  80%|███▏| 1.47G/1.83G [00:42<00:09, 38.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  81%|███▏| 1.48G/1.83G [00:42<00:08, 38.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  81%|███▎| 1.49G/1.83G [00:42<00:08, 38.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  82%|███▎| 1.50G/1.83G [00:43<00:08, 39.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  83%|███▎| 1.51G/1.83G [00:43<00:07, 39.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  83%|███▎| 1.52G/1.83G [00:43<00:07, 40.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  84%|███▎| 1.53G/1.83G [00:43<00:07, 39.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  84%|███▎| 1.54G/1.83G [00:44<00:07, 39.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  85%|███▍| 1.55G/1.83G [00:44<00:07, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  85%|███▍| 1.56G/1.83G [00:44<00:07, 37.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  86%|███▍| 1.57G/1.83G [00:45<00:06, 37.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  87%|███▍| 1.58G/1.83G [00:45<00:07, 33.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  87%|███▍| 1.59G/1.83G [00:45<00:06, 33.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  88%|███▌| 1.60G/1.83G [00:46<00:07, 31.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  88%|███▌| 1.61G/1.83G [00:46<00:07, 30.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  89%|███▌| 1.63G/1.83G [00:46<00:06, 31.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  89%|███▌| 1.64G/1.83G [00:47<00:06, 30.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  90%|███▌| 1.65G/1.83G [00:47<00:07, 24.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  91%|███▋| 1.66G/1.83G [00:48<00:06, 27.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  91%|███▋| 1.67G/1.83G [00:48<00:05, 28.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  92%|███▋| 1.68G/1.83G [00:48<00:05, 29.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  92%|███▋| 1.69G/1.83G [00:49<00:04, 31.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  93%|███▋| 1.70G/1.83G [00:49<00:03, 33.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  94%|███▋| 1.71G/1.83G [00:49<00:03, 35.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  94%|███▊| 1.72G/1.83G [00:49<00:02, 36.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  95%|███▊| 1.73G/1.83G [00:50<00:02, 37.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  95%|███▊| 1.74G/1.83G [00:50<00:02, 37.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  96%|███▊| 1.75G/1.83G [00:50<00:02, 37.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  96%|███▊| 1.76G/1.83G [00:50<00:01, 36.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  97%|███▉| 1.77G/1.83G [00:51<00:01, 36.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  98%|███▉| 1.78G/1.83G [00:51<00:01, 34.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  98%|███▉| 1.79G/1.83G [00:51<00:00, 35.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  99%|███▉| 1.80G/1.83G [00:52<00:00, 35.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin:  99%|███▉| 1.81G/1.83G [00:52<00:00, 36.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00007.bin: 100%|████| 1.83G/1.83G [00:52<00:00, 34.7MB/s]\u001b[A\n",
      "Fetching 1 files: 100%|███████████████████████████| 1/1 [00:54<00:00, 54.12s/it]\n",
      "/root/THUDM/chatglm3-6b\n"
     ]
    }
   ],
   "source": [
    "# 国内使用此命令镜像下载模型文件\n",
    "!export HF_ENDPOINT=https://hf-mirror.com && huggingface-cli download --resume-download --local-dir-use-symlinks False THUDM/chatglm3-6b --local-dir /root/THUDM/chatglm3-6b --include pytorch_model-00001-of-00007.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/lm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:05<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好👋！我是人工智能助手 ChatGLM3-6B，很高兴见到你，欢迎问我任何问题。\n",
      "晚上睡不着觉可能会让人感到焦虑和疲劳，这里有一些建议可能对你有所帮助：\n",
      "\n",
      "1. 尝试放松：深呼吸、冥想、渐进性肌肉松弛等放松技巧可以帮助你减轻焦虑和压力，促进睡眠。\n",
      "2. 规律作息：保持规律的作息时间，每天尽量在相同的时间上床睡觉和起床，有助于调整身体的生物钟。\n",
      "3. 减少使用电子设备：电子屏幕产生的蓝光可能会干扰你的睡眠，建议在睡前一小时停止使用电子设备。\n",
      "4. 创造一个良好的睡眠环境：保持安静、舒适、黑暗的睡眠环境，可能需要使用眼罩或耳塞等辅助工具。\n",
      "5. 避免饮用咖啡因和酒精：咖啡因和酒精都可能干扰睡眠，建议避免在晚上饮用。\n",
      "6. 进行适当的运动：适当的运动可以帮助你放松身心，但避免在睡前进行剧烈运动。\n",
      "7. 饮食调整：避免在睡前两三小时内摄入过多的食物和饮料，尤其是含咖啡因和糖分的食物。\n",
      "\n",
      "如果以上方法都不能改善你的失眠问题，可能需要考虑咨询专业的医生或心理学家，获取更具体的建议和治疗方案。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "model = AutoModel.from_pretrained(\"/root/THUDM/chatglm3-6b\", trust_remote_code=True, device='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/THUDM/chatglm3-6b\", trust_remote_code=True)\n",
    "\n",
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗\n",
    "### 英文Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given data has an error in the third column. The country \"China\" is associated with the city \"Tokyo\", which is incorrect. The correct country for Tokyo is \"Japan\". Therefore, the error in the data is 1 for this line. The updated data is:\n",
      "\n",
      "1,New York,United States\n",
      "2,London,United Kingdom\n",
      "3,Paris,France\n",
      "4,Tokyo,Japan\n",
      "5,Sydney,Australia\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Example output: [0,0,0,0,1]\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data provided is correct and does not contain any errors. Therefore, the desired output would be [0, 0, 1, 0, 1].\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Example output: [0,0,1,0,1]\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Example output: [0,0,1,0,1]\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given data has errors in the following line:\n",
      "4,Tokyo,China\n",
      "\n",
      "The country name \"China\" is incorrect, as Tokyo is actually located in Japan.\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Example output: [0,0,1,0,1]\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Example output: [0,0,1,0,1]\\n\\\n",
    "Notice: return the number list only, do not return any string or explaination.\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "dete = \"You need to help find out errors in given data. The first column of data is number, The second column of data is city, the third column of data is the conuntry which the second column belongs to.\\n\\\n",
    "Desired Format: list format with the result, result has only two values: 0 or 1, when you find an error in a line,the result is 1 , otherwise the result is 0 .\\n\\\n",
    "Notice: return the number list only, do not return any string or explaination.\\n\\\n",
    "Data:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def check_data(data):\n",
      "    result = []\n",
      "    for row in data:\n",
      "        if len(row) != 3:\n",
      "            result.append(1)\n",
      "        elif not (isinstance(row[0], int) and isinstance(row[1], str) and isinstance(row[2], str)):\n",
      "            result.append(1)\n",
      "        else:\n",
      "            result.append(0)\n",
      "    return result\n",
      "\n",
      "data = [\n",
      "    (1, \"New York\", \"United States\"),\n",
      "    (2, \"London\", \"United Kingdom\"),\n",
      "    (3, \"Paris\", \"France\"),\n",
      "    (4, \"Tokyo\", \"China\"),\n",
      "    (5, \"Sydney\", \"Australia\")\n",
      "]\n",
      "\n",
      "print(check_data(data))\n"
     ]
    }
   ],
   "source": [
    "dete = \"对给定的数据进行错误检查，数据的第一列是数字，第二列是城市，第三列是第二列所属的国家。\\n\\\n",
    "当某一行数据没有错误时，返回结果是0，当某一行数据有错误时，返回结果是1。\\n\\\n",
    "一个返回的例子: [0,0,1,0,1]\\n\\\n",
    "注意：最终结果只要返回一个数字组成的列表，不需要返回任何解释或者任何字符内容.\\n\\\n",
    "数据:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，我会对给定的数据进行错误检查。请提供您的数据。\n"
     ]
    }
   ],
   "source": [
    "dete = \"You are a helpful assisant. 你的任务是对给定的数据进行错误检查，数据的第一列是数字，第二列是城市，第三列是第二列所属的国家。\\n\\\n",
    "当某一行数据没有错误时，返回结果是0，当某一行数据有错误时，返回结果是1。\\n\\\n",
    "一个返回的例子: [0,0,1,0,1]\\n\\\n",
    "注意：最终结果只要返回一个数字组成的列表，不需要返回任何解释或者任何字符内容.\\n\\\n",
    "数据:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def check_data(data):\n",
      "    result = []\n",
      "    for row in data:\n",
      "        if len(row) != 3:\n",
      "            result.append(1)\n",
      "        elif not (isinstance(row[0], int) and isinstance(row[1], str) and isinstance(row[2], str)):\n",
      "            result.append(1)\n",
      "        else:\n",
      "            result.append(0)\n",
      "    return result\n"
     ]
    }
   ],
   "source": [
    "dete = \"You are a helpful assisant. 你的任务是对给定的数据进行错误检查，数据的第一列是数字，第二列是城市，第三列是第二列所属的国家。\\n\\\n",
    "当某一行数据没有错误时，返回结果是0，当某一行数据有错误时，返回结果是1。\\n\\\n",
    "一个返回的例子: [0,0,1,0,1]\\n\\\n",
    "注意：最终结果只要返回一个数字组成的列表，不需要返回任何解释或者任何字符内容.\\n\\\n",
    "数据:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def check_data(data):\n",
      "    result = []\n",
      "    for row in data:\n",
      "        if row[1] == 'United Kingdom':\n",
      "            result.append(0)\n",
      "        elif row[1] == 'France':\n",
      "            result.append(1)\n",
      "        else:\n",
      "            result.append(0)\n",
      "    return result\n",
      "\n",
      "data = [\n",
      "    [1, 'New York', 'United States'],\n",
      "    [2, 'London', 'United Kingdom'],\n",
      "    [3, 'Paris', 'France'],\n",
      "    [4, 'Tokyo', 'China'],\n",
      "    [5, 'Sydney', 'Australia']\n",
      "]\n",
      "\n",
      "print(check_data(data))\n"
     ]
    }
   ],
   "source": [
    "dete = \"You are a helpful assisant. 你的任务是对给定的数据进行错误检查，数据的第一列是数字，第二列是城市，第三列是第二列所属的国家。\\n\\\n",
    "数据如下:\\n\\\n",
    "1,New York,United States\\n\\\n",
    "2,London,United Kingdom\\n\\\n",
    "3,Paris,France\\n\\\n",
    "4,Tokyo,China\\n\\\n",
    "5,Sydney,Australia\\n\\\n",
    "当某一行数据没有错误时，返回结果是0，当某一行数据有错误时，返回结果是1。\\n\\\n",
    "一个返回的例子: [0,0,1,0,1]\\n\\\n",
    "注意：最终结果只要返回一个数字组成的列表，不需要返回任何解释或者任何字符内容.\"\n",
    "response, history = model.chat(tokenizer, dete, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "可以完成简单的清洗任务，能正确返回detection的结果与解释。\n",
    "但不能完全理解任务要求，也有可能是prompt的原因，返回结果不合要求。\n",
    "中文prompt甚至直接返回代码。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
